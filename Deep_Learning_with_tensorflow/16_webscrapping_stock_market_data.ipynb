{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow;font-weight:bold;text-decoration:underline; font-size:50px\">Stock market data scrapping</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why stock market data scrapping is important?\n",
    "\n",
    "Stock market data scraping is important because it allows traders and investors to gather large amounts of data from various sources, analyze it, and make informed decisions based on the insights gained. This can incude tracking stocks prices, analyzing market trends, and monitoring news and socila media sentiment. Additionally, sotck market data scraping can help trends, and investors identyfy potential risks and opportunites, and make more informed decisons about when to buy or sell stocks. \n",
    "\n",
    "\n",
    "\n",
    "There are several libraries athan can be use to scrape stock market data in Python. Some poplar ones include:\n",
    "\n",
    "1. BeautifulSoup\n",
    "2. Scrapy\n",
    "3. Selenium\n",
    "4. Pandas\n",
    "5. Requests \n",
    "\n",
    "Each of these libraries ahs its own strengths and weaknesses, and the choice of which one to use will depend on the specific requirements of your porject. For Example , if you need to scrape data from dynamic websites that require user interaction, you may wat ot use Selenium, If you need to scrape data from muliple pages or websites, Scrapy may be a good choice. If you need to manipulate and analyze the data after scraping it. Pands my be the way to go. \n",
    "\n",
    "\n",
    "\n",
    "## Example Code for Scraping Stock Prices \n",
    "\n",
    "\n",
    "Here's an example of how to use yfinance to get the stock price of Apple (AAPL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.36-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (1.26.3)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting lxml>=4.9.1 (from yfinance)\n",
      "  Downloading lxml-5.1.0-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting appdirs>=1.4.4 (from yfinance)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yfinance) (2023.3.post1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.0.tar.gz (314 kB)\n",
      "     ---------------------------------------- 0.0/314.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/314.6 kB ? eta -:--:--\n",
      "     ----- --------------------------------- 41.0/314.6 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------- ---------------------------- 81.9/314.6 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 163.8/314.6 kB 1.6 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 163.8/314.6 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  307.2/314.6 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 314.6/314.6 kB 1.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.17.1.tar.gz (3.0 MB)\n",
      "     ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 0.2/3.0 MB 3.1 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.3/3.0 MB 2.6 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.3/3.0 MB 2.6 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.3/3.0 MB 1.4 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.3/3.0 MB 1.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.5/3.0 MB 1.6 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.5/3.0 MB 1.6 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.6/3.0 MB 1.5 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.6/3.0 MB 1.4 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.7/3.0 MB 1.4 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.7/3.0 MB 1.4 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.8/3.0 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.8/3.0 MB 1.4 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.9/3.0 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.9/3.0 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 1.0/3.0 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.0/3.0 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.1/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 1.1/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.2/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.2/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 1.3/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 1.3/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.4/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.4/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 1.5/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 1.5/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.6/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.6/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 1.7/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 1.7/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 1.7/3.0 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 1.8/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.9/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.9/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.0/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.1/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 2.1/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.2/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.2/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 2.3/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.3/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.3/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 2.4/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.4/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.5/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.5/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 2.7/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 2.7/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.8/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.8/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.9/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.9/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.9/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.9/3.0 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.9/3.0 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.0/3.0 MB 1.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting beautifulsoup4>=4.11.1 (from yfinance)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting html5lib>=1.1 (from yfinance)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "     ---------------------------------------- 0.0/112.2 kB ? eta -:--:--\n",
      "     -------------------------------- ------ 92.2/112.2 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 112.2/112.2 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Collecting webencodings (from html5lib>=1.1->yfinance)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saurabh kumar\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.31->yfinance) (2023.11.17)\n",
      "Downloading yfinance-0.2.36-py2.py3-none-any.whl (72 kB)\n",
      "   ---------------------------------------- 0.0/72.4 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/72.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 72.4/72.4 kB 796.9 kB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 112.6/147.9 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 112.6/147.9 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 147.9/147.9 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading lxml-5.1.0-cp311-cp311-win_amd64.whl (3.9 MB)\n",
      "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/3.9 MB 3.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.2/3.9 MB 2.0 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.2/3.9 MB 2.0 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.2/3.9 MB 2.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.3/3.9 MB 1.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.3/3.9 MB 1.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.4/3.9 MB 1.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.4/3.9 MB 1.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.5/3.9 MB 1.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.5/3.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.6/3.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.6/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.7/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.7/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.7/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.8/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.8/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.9/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.9/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.0/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.0/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.1/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.2/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.2/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.2/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.3/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.3/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.4/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.4/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.5/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.5/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.7/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.7/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 1.7/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.8/3.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.8/3.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.9/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.0/3.9 MB 1.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.1/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.1/3.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.2/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.2/3.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.3/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.3/3.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.3/3.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.4/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.4/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.5/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.5/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.6/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.7/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.8/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.8/3.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.9/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.9/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.9/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.0/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.0/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.1/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.2/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.3/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.4/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.4/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.5/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.5/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.5/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.6/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.6/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.7/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/3.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.9/3.9 MB 1.1 MB/s eta 0:00:00\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: frozendict, peewee\n",
      "  Building wheel for frozendict (pyproject.toml): started\n",
      "  Building wheel for frozendict (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for frozendict: filename=frozendict-2.4.0-py3-none-any.whl size=15471 sha256=f58cfbb6c4204ece6479fe1f85af33e5f2aaba59e4762da8b0fdec2abad0c048\n",
      "  Stored in directory: c:\\users\\saurabh kumar\\appdata\\local\\pip\\cache\\wheels\\31\\dd\\81\\a814e6f8cde8a1bbc1f088fdc273943371f10478b91a605e14\n",
      "  Building wheel for peewee (pyproject.toml): started\n",
      "  Building wheel for peewee (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.17.1-py3-none-any.whl size=136948 sha256=932423e80ee1a00570c518d1cea36c235dc8b778bac308a35ca30f73a1a83c20\n",
      "  Stored in directory: c:\\users\\saurabh kumar\\appdata\\local\\pip\\cache\\wheels\\33\\d2\\ca\\79b9807826bc7ef0b86a1ee28c372daaf073f9aa8756eedd7f\n",
      "Successfully built frozendict peewee\n",
      "Installing collected packages: webencodings, peewee, multitasking, appdirs, soupsieve, lxml, html5lib, frozendict, beautifulsoup4, yfinance\n",
      "Successfully installed appdirs-1.4.4 beautifulsoup4-4.12.3 frozendict-2.4.0 html5lib-1.1 lxml-5.1.0 multitasking-0.0.11 peewee-3.17.1 soupsieve-2.5 webencodings-0.5.1 yfinance-0.2.36\n"
     ]
    }
   ],
   "source": [
    "# !pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.96240234375\n"
     ]
    }
   ],
   "source": [
    "# Define the ticker symbol\n",
    "tickerSymbol = 'AAPL'\n",
    "\n",
    "# Get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "\n",
    "# Get the stock price history\n",
    "tickerDf = tickerData.history(period='1d', start='2021-01-01', end='2021-12-31')\n",
    "\n",
    "# Get the last closing price\n",
    "lastPrice = tickerDf['Close'].iloc[-1]\n",
    "\n",
    "# Print the last closing price\n",
    "print(lastPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the yfinance library to get data on the AAPL ticker, including its stock price history. It then extracts the last closing price from the history and prints it to the console. Note that you can adjust the start and end dates to get the stock price history for a different time period.\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:yellow;font-weight:bold;text-decoration:underline; font-size:50px\">Web Scrapping vs. Web Crawling</span> \n",
    "\n",
    "\n",
    "\n",
    "`Web scraping` and `web crawling` are two related but distincet techniques for gathering data from the web. \n",
    "\n",
    "`Web scraping` involves `extracting specific data from web pages` typically uisng tools like `BeautifulSoup` or `Scrapy` in Python. \n",
    "\n",
    " - This data can be using for a variety ofpurposes, such as analyzing market trends, monitoring social media sentiment, or gathering product information for price comparison websites. \n",
    "  \n",
    "`Web Crawling` on the other hand involves systematically exploring the web to gather data typically using authmate dbolts or spiders. \n",
    "\n",
    " - This data can be used to create search engine indexes, monitor website changes, or gather data form academic reserach. \n",
    "\n",
    "\n",
    "In summary, `web scraping is focused on extrcting specific data form web pages`, while `web crawling is focused on systematicallly exploring the web to gather data.` \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-weight:bold;text-decoration:underline; font-size:30px\">Challenges scrapping stockmarket data</span> \n",
    "\n",
    "\n",
    "There are several common challenges when scraping stock market data, including:\n",
    "\n",
    "1. `Dynamic websites:` Many stock market websites use dynamic content that is generated by JavaScript, wich cna make it difficult to scrape the data using traditional web scraping techniques. In these cases, you may need to use a tool like selenium to automate a web browser and iteract with the wiebsite in order to scrape the data. \n",
    "   \n",
    "2. `Anti-scraping measurea:` Some websites may have anti-scraping measures in place to prevent automated scraping. These measures can include CAPTCHAs. IP blocking, or user agent detection. To avoid these measures, you may nedd to ause techniques like rotationg IP addresses or user agents, or uisng a proxy server. \n",
    "\n",
    "3. `Data formatting:` Stock market data can be presented in a variety of formats including table,s cahrts, and graphs. Extracing the data from these formats can be challenging, and may require specialized tools or techniques. \n",
    "\n",
    "4. `Data quality:`  Stock makret data can be noisy and contain errors or outliers. It's important to carefully clean and validata the data before using it for analysis or decision-making. \n",
    "\n",
    "5. `Legal and ethical considrations:` Scrapping stock market data can raise legal and ethicla concerns, particularly if the data is used for insider trading or other illegal activites. It's activites. It's important to ensure that your scraping activites are legal and ethicla, and to obtian any necessary permissions or licenses before scraping data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-weight:bold;text-decoration:underline; font-size:30px\">Techniques ot avoid anti-scraping measures whe scraping stock market data ?</span> \n",
    "\n",
    "There are several techniques that can be used to avoid anti-scraping measures when scraping stock market data, including:\n",
    "\n",
    "1.  Use a proxy server: A proxy server can be used to route your requests through a different IP address, which cna help yo0u avoid IP blocking. There are aseveral free adn paid proxy services available that you can use. \n",
    "2. Rotate user agents. Som webiste may block requestas form certin user agents, so rotating you user agent can help you avoid detection. You can user a library like `fake_useragent` in Python to generate random user agents for each request.\n",
    "3. Slow  down your requests: sending too many requests too quckly can trigger anti-scraping measures, aso slowing dow your requests cna help you avoid detection. You can use a library like `time` in Python to add a delay betwwn each request. \n",
    "4. Use CAPTCHA solving services: Some websites may require you to solve a CAPTCHA in order to access the data. There are serveral CAPTCHA solving services available that you can user to automate this proces. \n",
    "5. User ahndless browsers; some websites may use JavaScript to generate content, which cna make it difficult to scrape the data using traditional web scraping techniques. using a headless browser like Selenium cna help you autoamte the process of interacting with the website ans xraping the data. \n",
    "\n",
    "\n",
    "It's importanta to note that while these techniques cna help you avid anti-scring measures, they may not be foolproof and may still result in you request being blocked you your Ip address being banned. It's always a good idea to chek the websit's terms of srvices and to be respectul of ther policies when scraping data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-weight:bold;text-decoration:underline; font-size:30px\"> What are some common data formatting issues when scraping stock market data? </span>\n",
    "\n",
    "\n",
    "\n",
    "There are several common data formatting issues when scraping stock market data, including:\n",
    "\n",
    "1. Inconsistent data types: Stock market data can be presented in a variety of formats, including text, numbers, and dates. It's important to ensure that the data is consistently formatted and that the data types are correct before using it for analysis.\n",
    "\n",
    "2. Missing data: Stock market data can be incomplete or missing, which can make it difficult to analyze. It's important to handle missing data appropriately, either by imputing missing values or by excluding them from the analysis.\n",
    "\n",
    "3. Non-standard data formats: Some stock market data may be presented in non-standard formats, such as PDFs or images. Extracting data from these formats can be challenging and may require specialized tools or techniques.\n",
    "\n",
    "4. Data normalization: Stock market data can be presented in different units or currencies, which can make it difficult to compare across different stocks or markets. It's important to normalize the data to a common unit or currency before using it for analysis.\n",
    "\n",
    "5. Data cleaning: Stock market data can be noisy and contain errors or outliers. It's important to carefully clean and validate the data before using it for analysis or decision-making.\n",
    "\n",
    "It's important to be aware of these formatting issues when scraping stock market data and to take steps to address them before using the data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span ></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's scrape some stock market data! \n",
    "\n",
    "Sure! Here's an example code snppet that uses yfinance to downlod the sotcke data of Google (GOOGL) for the last year today and stores it in a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Open        High         Low       Close  \\\n",
      "Date                                                                        \n",
      "2024-02-12 00:00:00-05:00  148.419998  149.339996  147.369995  147.529999   \n",
      "2024-02-13 00:00:00-05:00  144.919998  146.669998  143.690002  145.139999   \n",
      "2024-02-14 00:00:00-05:00  146.080002  146.520004  144.089996  145.940002   \n",
      "2024-02-15 00:00:00-05:00  143.139999  143.520004  140.460007  142.770004   \n",
      "2024-02-16 00:00:00-05:00  142.990005  143.190002  140.139999  140.520004   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2024-02-12 00:00:00-05:00  21564100        0.0           0.0  \n",
      "2024-02-13 00:00:00-05:00  27837700        0.0           0.0  \n",
      "2024-02-14 00:00:00-05:00  22704200        0.0           0.0  \n",
      "2024-02-15 00:00:00-05:00  37590700        0.0           0.0  \n",
      "2024-02-16 00:00:00-05:00  31451100        0.0           0.0  \n",
      "                                Open       High        Low      Close  \\\n",
      "Date                                                                    \n",
      "2023-02-17 00:00:00-05:00  94.849998  95.559998  93.209999  94.349998   \n",
      "2023-02-21 00:00:00-05:00  93.000000  93.099998  91.720001  91.790001   \n",
      "2023-02-22 00:00:00-05:00  91.699997  92.110001  90.610001  91.650002   \n",
      "2023-02-23 00:00:00-05:00  91.919998  91.940002  89.760002  90.889999   \n",
      "2023-02-24 00:00:00-05:00  89.440002  89.889999  88.580002  89.129997   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2023-02-17 00:00:00-05:00  34284100        0.0           0.0  \n",
      "2023-02-21 00:00:00-05:00  33629300        0.0           0.0  \n",
      "2023-02-22 00:00:00-05:00  30884000        0.0           0.0  \n",
      "2023-02-23 00:00:00-05:00  41206400        0.0           0.0  \n",
      "2023-02-24 00:00:00-05:00  36585100        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Define the ticker symbol\n",
    "tickerSymbol = 'GOOGL'\n",
    "\n",
    "# Get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "\n",
    "# Get the stock price history\n",
    "tickerDf = tickerData.history(period='1y')\n",
    "\n",
    "# Print the last 5 rows of the DataFrame\n",
    "print(tickerDf.tail())\n",
    "print(tickerDf.head())\n",
    "# Save the DataFrame to a CSV file\n",
    "# tickerDf.to_csv('googl_stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickerDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Open      High       Low     Close     Volume  \\\n",
      "Date                                                                           \n",
      "2010-06-29 00:00:00-04:00  1.266667  1.666667  1.169333  1.592667  281494500   \n",
      "2010-06-30 00:00:00-04:00  1.719333  2.028000  1.553333  1.588667  257806500   \n",
      "2010-07-01 00:00:00-04:00  1.666667  1.728000  1.351333  1.464000  123282000   \n",
      "2010-07-02 00:00:00-04:00  1.533333  1.540000  1.247333  1.280000   77097000   \n",
      "2010-07-06 00:00:00-04:00  1.333333  1.333333  1.055333  1.074000  103003500   \n",
      "\n",
      "                           Dividends  Stock Splits  \n",
      "Date                                                \n",
      "2010-06-29 00:00:00-04:00        0.0           0.0  \n",
      "2010-06-30 00:00:00-04:00        0.0           0.0  \n",
      "2010-07-01 00:00:00-04:00        0.0           0.0  \n",
      "2010-07-02 00:00:00-04:00        0.0           0.0  \n",
      "2010-07-06 00:00:00-04:00        0.0           0.0  \n",
      "(2410, 7)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# define ticker symbol\n",
    "tickerSymbol = 'TSLA'\n",
    "\n",
    "# get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "# tickerData.info\n",
    "\n",
    "# get the historical prices for this ticker\n",
    "tickerDf = tickerData.history(period='1d', start='2010-1-1', end='2020-1-25')\n",
    "\n",
    "# last closing price\n",
    "tickerDf['Close'].iloc[-1]\n",
    "\n",
    "print(tickerDf.head())\n",
    "print(tickerDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-17 00:00:00-05:00</th>\n",
       "      <td>170.220001</td>\n",
       "      <td>173.179993</td>\n",
       "      <td>169.699997</td>\n",
       "      <td>172.880005</td>\n",
       "      <td>24171300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-21 00:00:00-05:00</th>\n",
       "      <td>174.309998</td>\n",
       "      <td>178.169998</td>\n",
       "      <td>171.880005</td>\n",
       "      <td>172.080002</td>\n",
       "      <td>34592600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-22 00:00:00-05:00</th>\n",
       "      <td>171.070007</td>\n",
       "      <td>172.759995</td>\n",
       "      <td>169.690002</td>\n",
       "      <td>171.119995</td>\n",
       "      <td>22433200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-23 00:00:00-05:00</th>\n",
       "      <td>172.000000</td>\n",
       "      <td>173.690002</td>\n",
       "      <td>169.380005</td>\n",
       "      <td>172.039993</td>\n",
       "      <td>20017800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-24 00:00:00-05:00</th>\n",
       "      <td>168.639999</td>\n",
       "      <td>170.720001</td>\n",
       "      <td>167.660004</td>\n",
       "      <td>170.389999</td>\n",
       "      <td>19791300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2023-02-17 00:00:00-05:00  170.220001  173.179993  169.699997  172.880005   \n",
       "2023-02-21 00:00:00-05:00  174.309998  178.169998  171.880005  172.080002   \n",
       "2023-02-22 00:00:00-05:00  171.070007  172.759995  169.690002  171.119995   \n",
       "2023-02-23 00:00:00-05:00  172.000000  173.690002  169.380005  172.039993   \n",
       "2023-02-24 00:00:00-05:00  168.639999  170.720001  167.660004  170.389999   \n",
       "\n",
       "                             Volume  Dividends  Stock Splits  \n",
       "Date                                                          \n",
       "2023-02-17 00:00:00-05:00  24171300        0.0           0.0  \n",
       "2023-02-21 00:00:00-05:00  34592600        0.0           0.0  \n",
       "2023-02-22 00:00:00-05:00  22433200        0.0           0.0  \n",
       "2023-02-23 00:00:00-05:00  20017800        0.0           0.0  \n",
       "2023-02-24 00:00:00-05:00  19791300        0.0           0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "today = date.today()\n",
    "d1 = today.strftime(\"%Y-%m-%d\")\n",
    "d1\n",
    "d2 = (today - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "d2\n",
    "start_date = d2\n",
    "end_date = d1\n",
    "\n",
    "# define ticker symbol\n",
    "tickerSymbol = 'META'\n",
    "\n",
    "# get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "# tickerData.info\n",
    "\n",
    "# get the historical prices for this ticker\n",
    "tickerDf = tickerData.history(period='1d', start=start_date, end=end_date)\n",
    "tickerDf.head()\n",
    "\n",
    "# tickerDf.to_csv('META.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
