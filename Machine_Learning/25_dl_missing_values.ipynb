{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Deep Learning Methods \n",
    "\n",
    "Neural networks especially autoemcoders can be effective in imputing missing values in complex dataets. \n",
    "deep learing methods particularly neural networks liek autnencodrs, offer a powerful approach for imputing missing values in comple datasets. These methods are especially useful when the data has intericate non-liner relationships that trditional statistical methdos migh not captuere effectively. \n",
    "\n",
    "\n",
    "\n",
    "#### Undersatnding Autoencoders for Imputation:\n",
    "\n",
    "1. **What is an Autoencoder?**\n",
    "    - An autoencoder is a type of neural network that is trained to copy its input to its output. \n",
    "    - It has a hidden layer that describes a code used to represent the input \n",
    "    - The network may be viewed as consisting of two parts: an endocer function which compressses the nput nto a latent-space representatino and a decoder function, which reconsitructs the input from the latent space.\n",
    "\n",
    "2. **How Autoencoders Work for Imputation:\n",
    "**\n",
    "- The key idea is to train the autoencoder to ignore the noise (missing values) in the input data.\n",
    "- During training, inputs with missing values are presented, and the network learns to predict the missing values in a way that minimizes reconstruction error for known parts of the data.\n",
    "- This results in the network learning a robust representation of the data, enabling it to make reasonable guesses about missing values.\n",
    "Advantages of Using Autoencoders:\n",
    "\n",
    "3. **Handling Complex Patterns:** \n",
    "- They can capture non-linear relationships in the data, which is particularly useful for complex datasets.\n",
    "- **Scalability:** They can handle large-scale datasets efficiently.\n",
    "- **Flexibility:** They can be adapted to different types of data (e.g., images, text, time-series).\n",
    "Implementation Considerations:\n",
    "\n",
    "4. **Data Preprocessing:** \n",
    "- Data should be normalized or standardized before feeding it into an autoencoder.\n",
    "- Network Architecture: The choice of architecture (number of layers, type of layers, etc.) depends on the complexity of the data.\n",
    "- **Training Process:** It might involve techniques like dropout or noise addition to improve the model's ability to handle missing data.\n",
    "\n",
    "5. **Example Use-Cases:**\n",
    "\n",
    "- **Image Data:** Filling in missing pixels or reconstructing corrupted images.\n",
    "- **Time-Series Data:** Imputing missing values in sequences like stock prices or weather data.\n",
    "- **Tabular Data:** Handling missing entries in datasets used for machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementtation Example:\n",
    "Here's simplified example fo how you might set up an autoencoder for imputationin Python using TensorFlow and keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Saurabh Kumar\\miniconda3\\envs\\tf_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense \n",
    "from tensorflow.keras.models import Model \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset \n",
    "df_titanic = sns.load_dataset('titanic') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectin relevant features for simplicity \n",
    "df_titanic = df_titanic[['survived','pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "#separate features aond target \n",
    "X = df_titanic.drop('survived', axis=1)\n",
    "y = df_titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handlingmissign values and categorical variables \n",
    "numeric_features = ['age', 'fare', 'sibsp', 'parch']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('impter', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# ColumnTransformer for preprocessing \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features), \n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the datset \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
